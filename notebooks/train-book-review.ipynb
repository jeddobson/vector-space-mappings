{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "from glob import glob\n",
    "\n",
    "# core nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# gensim magic\n",
    "import gensim\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for line in open('../data/goodreads_reviews_spoiler.json', 'r'):\n",
    "    reviews.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select just review text\n",
    "reviews = [x['review_sentences'] for x in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now extract just \n",
    "extracted_reviews = list()\n",
    "for r in reviews:\n",
    "    text = str()\n",
    "    for j in r:\n",
    "        text = text + \" \" + j[1]\n",
    "    extracted_reviews.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=list()\n",
    "for r in extracted_reviews:\n",
    "    tokens = word_tokenize(r)    \n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    tmp_text=list()\n",
    "\n",
    "    for word in tokens:\n",
    "        if len(word) == 1:\n",
    "            if word.isalpha == True:\n",
    "                tmp_text.append(word)\n",
    "        else:\n",
    "             tmp_text.append(word)           \n",
    "    tokens = tmp_text\n",
    "\n",
    "    # now remove leading and trailing quotation marks,      \n",
    "    # hyphens and  dashes\n",
    "    tmp_text=list()\n",
    "    drop_list = ['“','\"','”','-','—']\n",
    "    for word in tokens:\n",
    "        if word[0] in drop_list:\n",
    "            word = word[1:]\n",
    "        if word[-1:] in drop_list:\n",
    "            word = word[:-1]\n",
    "\n",
    "    # catch any zero-length words remaining\n",
    "        if len(word) > 0:\n",
    "            tmp_text.append(word)\n",
    "    tokens = tmp_text\n",
    "    reviews.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source documents\n",
    "# dimension of feature vectors \n",
    "# max distance   \n",
    "# number of times a word must appear to be included in vocab\n",
    "# for parallelization\n",
    "\n",
    "print(\"starting training...\")\n",
    "book_review_model = gensim.models.Word2Vec(\n",
    "    reviews, \n",
    "    sg=0,           # sg=1 is use skip-gram, sg=0 is cbow \n",
    "    size=200,        \n",
    "    window=15,     \n",
    "    min_count=2,    # increase to limit vocab and find fewer rare words\n",
    "    workers=10,     \n",
    "    iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"saving output\")\n",
    "fp = open(\"../models/book-reviews-vectors.w2v\",'wb')\n",
    "book_review_model.wv.save(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_review_model.wv.most_similar(\"l\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
